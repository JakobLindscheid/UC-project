{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import folium\n",
    "import shapely\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from osgeo import gdal\n",
    "import geopandas as gpd\n",
    "gdal.SetConfigOption('SHAPE_RESTORE_SHX', 'YES')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POI categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(\"C:/Users/Public/fsq/fsq.db\")\n",
    "con.sql(\n",
    "    \"CREATE TABLE IF NOT EXISTS places AS SELECT * FROM read_parquet( 'C:/Users/Public/fsq/places-*.snappy.parquet');\"\n",
    ")\n",
    "con.sql(\n",
    "    \"create TABLE IF NOT EXISTS categories as SELECT * FROM read_parquet( 'C:/Users/Public/fsq/categories.snappy.parquet');\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_earth = 6_371_000 # meters\n",
    "def move_lat(lat, d_lat):\n",
    "    lat += d_lat / r_earth * 180 / np.pi\n",
    "    return lat\n",
    "def move_long(long, d_long, lat):\n",
    "    long += d_long / r_earth * 180 / np.pi / np.cos(lat * np.pi / 180)\n",
    "    return long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_squares(min_lat, max_lat, min_long, max_long, square_size=500):\n",
    "    long = min_long\n",
    "    lat = min_lat\n",
    "\n",
    "    columns={\"index1\":[], \"index2\":[], \"min_long\":[], \"min_lat\":[], \"max_long\":[], \"max_lat\":[]}\n",
    "\n",
    "    i = 0\n",
    "    while lat <= max_lat:\n",
    "        next_lat = move_lat(lat, square_size)\n",
    "\n",
    "        j = 0\n",
    "        while long <= max_long:\n",
    "            next_long = move_long(long, square_size, lat)\n",
    "\n",
    "            columns[\"index1\"].append(i)\n",
    "            columns[\"index2\"].append(j)\n",
    "            columns[\"min_long\"].append(long)\n",
    "            columns[\"min_lat\"].append(lat)\n",
    "            columns[\"max_long\"].append(next_long)\n",
    "            columns[\"max_lat\"].append(next_lat)\n",
    "\n",
    "            long = next_long\n",
    "            j += 1\n",
    "            \n",
    "        i += 1\n",
    "        lat = next_lat\n",
    "        long = min_long    \n",
    "\n",
    "    squares = pd.DataFrame(columns)\n",
    "    return squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_pois(con, boundaries, categories, city_names=[\"Shanghai\", \"上海\"]):\n",
    "    min_lat, max_lat, min_long, max_long = boundaries\n",
    "    cat_condition = \" OR \".join([f\"\"\"level{level}_category_name = '{name.replace(\"'\", \"''\")}'\"\"\" for level, name in categories.values()])\n",
    "    df = con.sql(f\"\"\"\n",
    "            WITH filtered_places AS (\n",
    "                SELECT * FROM places WHERE ({\" OR \".join([\"'\" + city_name + \"' IN locality\" for city_name in city_names])})\n",
    "            ), \n",
    "            exploded_categories AS (\n",
    "                SELECT DISTINCT fsq_place_id\n",
    "                FROM (\n",
    "                    SELECT fsq_place_id,\n",
    "                        UNNEST(fsq_category_ids) as fsq_category_id\n",
    "                    FROM filtered_places\n",
    "                ) as p \n",
    "                JOIN categories AS c \n",
    "                ON p.fsq_category_id = c.category_id\n",
    "                \n",
    "                WHERE {cat_condition}\n",
    "            )\n",
    "            SELECT * FROM places as p JOIN exploded_categories as c ON p.fsq_place_id = c.fsq_place_id\n",
    "            WHERE latitude <= {max_lat} AND latitude >= {min_lat} AND longitude <= {max_long} AND longitude >= {min_long};\n",
    "            \"\"\"\n",
    "    ).df()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poi_counts(con,  squares, categories, city_names, boundaries):\n",
    "    \n",
    "    df = query_pois(con, boundaries, categories, city_names)\n",
    "\n",
    "    # filter out the data that is not in the area of interest\n",
    "    \"\"\" df = df[\n",
    "        (df[\"latitude\"] < max_lat) & \n",
    "        (df[\"latitude\"] > min_lat) &\n",
    "        (df[\"longitude\"] < max_long) &\n",
    "        (df[\"longitude\"] > min_long)\n",
    "    ] \"\"\"\n",
    "        \n",
    "    df[\"square\"] = df.apply(lambda x:\n",
    "        squares[\n",
    "            (squares[\"min_long\"]<= x[\"longitude\"]) & (squares[\"max_long\"]>x[\"longitude\"]) &\n",
    "            (squares[\"min_lat\"]<= x[\"latitude\"]) & (squares[\"max_lat\"]>x[\"latitude\"])\n",
    "        ].index[0], \n",
    "    axis=1)\n",
    "    \n",
    "    df = df.assign(category=\"\")\n",
    "    for cat in tqdm(categories.keys()):\n",
    "        df.loc[\n",
    "            df[df[\"fsq_category_labels\"].astype(str).str.contains(categories[cat][1])].index,\n",
    "            \"category\"\n",
    "        ] = cat\n",
    "    \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {\n",
    "    # 6/12 mentioned in the paper\n",
    "    \"education\": (2, 'Education'),\n",
    "    \"scenic\": (1, \"Landmarks and Outdoors\"),\n",
    "    \"sports\": (1,\"Sports and Recreation\"),\n",
    "    \"commercial\": (1,\"Retail\"),\n",
    "    \"financial\": (2,\"Financial Service\"),\n",
    "    \"transport\": (2,\"Transport Hub\"),\n",
    "    # added by us\n",
    "    \"entertainment\": (1, \"Arts and Entertainment\"),\n",
    "    \"office\": (2, \"Office\"),\n",
    "    \"government\": (2, \"Government Building\"),\n",
    "    \"food\": (2, \"Restaurant\"),\n",
    "    \"health\": (1, \"Health and Medicine\"),\n",
    "    \"hotel\": (2, \"Lodging\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {\n",
    "    c.lower().replace(\" \",\"_\"): (2, c)\n",
    "    for c in pd.read_csv(\"foursquare_categories.csv\")[\"category_label\"].str.split(\" > \", expand=True)[1].unique()\n",
    "    if c is not None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = {\n",
    "    \"Shanghai\": {\n",
    "        \"city_names\": [\"Shanghai\", \"上海\"],\n",
    "        \"shape_file\": 'shanghai-provjson.shp',\n",
    "    },\n",
    "    \"Nanjing\": {\n",
    "        \"city_names\": [\"Nanjing\", \"南京\"],\n",
    "        \"corner1\": (118.39246295229297, 31.261649948659116),\n",
    "        \"corner2\": (119.22570190410435, 32.56306709606652)\n",
    "    },\n",
    "    \"Beijing\": {\n",
    "        \"city_names\": [\"Beijing\", \"北京\"],\n",
    "        \"corner1\": (115.39783664651948, 39.45697643649945),\n",
    "        \"corner2\": (117.59107359664718, 41.053361082863574), \n",
    "    },\n",
    "    \"Xi'An\": {\n",
    "        \"city_names\": [\"Xi''an\", \"西安\"],\n",
    "        \"corner1\": (107.58696095508553, 33.684721810688416),\n",
    "        \"corner2\": (109.828485064734, 34.833361874606794), \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 427/427 [03:12<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shanghai 63991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 427/427 [00:34<00:00, 12.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nanjing 11185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 427/427 [02:48<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beijing 55453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 427/427 [00:21<00:00, 20.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xi'An 6545\n"
     ]
    }
   ],
   "source": [
    "full_dataset = []\n",
    "for city in cities.keys():\n",
    "\n",
    "    # get the boundaries of the city\n",
    "    shape_file = cities[city].get(\"shape_file\")\n",
    "    if shape_file:\n",
    "        geo_df = gpd.read_file(shape_file)\n",
    "\n",
    "        polygons = geo_df.geometry.tolist()\n",
    "        boundary = gpd.GeoSeries(shapely.ops.unary_union(polygons))\n",
    "        long, lat = boundary.at[0].exterior.coords.xy\n",
    "\n",
    "        min_long = min(long)\n",
    "        max_long = max(long)\n",
    "        min_lat = min(lat)\n",
    "        max_lat = max(lat)\n",
    "\n",
    "    else:\n",
    "        corner1 = cities[city].get(\"corner1\")\n",
    "        corner2 = cities[city].get(\"corner2\")\n",
    "        if corner1 is None or corner2 is None:\n",
    "            raise ValueError(\"Please provide either a shape file or the top left and bottom right coordinates.\")\n",
    "        \n",
    "        min_long, min_lat = corner1\n",
    "        max_long, max_lat = corner2\n",
    "    \n",
    "    # generate the squares\n",
    "    squares = generate_squares(min_lat, max_lat, min_long, max_long, 1000)\n",
    "\n",
    "    # get the poi counts\n",
    "    data = poi_counts(con, squares, categories, cities[city][\"city_names\"], (min_lat, max_lat, min_long, max_long))\n",
    "    data = data.drop_duplicates(subset=[\"latitude\", \"longitude\", \"name\"])\n",
    "    print(city, len(data))\n",
    "    \n",
    "    # merge the data with the squares\n",
    "    squares = pd.merge(\n",
    "        squares, \n",
    "        data.pivot_table(index=\"square\", columns=\"category\", values=\"name\", aggfunc=\"count\", fill_value=0), \n",
    "        left_index=True, \n",
    "        right_index=True, \n",
    "        how=\"outer\", \n",
    "    ).fillna(0)\n",
    "\n",
    "    squares = squares.assign(label=city)\n",
    "    full_dataset.append(squares)\n",
    "full_dataset = pd.concat(full_dataset, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = full_dataset.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = full_dataset.loc[:, [c for c in full_dataset.columns if c != \"label\"] + [\"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = full_dataset[(full_dataset[full_dataset.columns[6:-1]]!=0).any(axis=1)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Check-ins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_poi = pd.read_csv(r\"C:\\Users\\Public\\LSBN2vec++_dataset_WWW2019\\dataset_WWW2019\\raw_POIs.txt\", delimiter=\"\\t\", header=None)\n",
    "checkin_poi.columns = [\"id\", \"latitude\", \"longitude\", \"category\", \"country\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_poi = checkin_poi[checkin_poi[\"country\"]==\"CN\"].reset_index(drop=True)\n",
    "checkin_poi[\"square\"] = checkin_poi.apply(lambda x:\n",
    "    full_dataset[\n",
    "        (full_dataset[\"min_long\"]<= x[\"longitude\"]) & (full_dataset[\"max_long\"]>x[\"longitude\"]) &\n",
    "        (full_dataset[\"min_lat\"]<= x[\"latitude\"]) & (full_dataset[\"max_lat\"]>x[\"latitude\"])\n",
    "    ].index, \n",
    "axis=1)\n",
    "checkin_poi[\"square\"] = checkin_poi.apply(lambda x: x[\"square\"][0] if len(x[\"square\"]) > 0 else np.nan, axis=1)\n",
    "checkin_poi = checkin_poi.dropna(subset=[\"square\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkins = pd.read_csv(r\"C:\\Users\\Public\\LSBN2vec++_dataset_WWW2019\\dataset_WWW2019\\raw_Checkins_anonymized.txt\", delimiter=\"\\t\", header=None)\n",
    "checkins.columns = [\"user\", \"poi\", \"datetime\", \"tz\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkins = checkins[checkins[\"poi\"].isin(checkin_poi[\"id\"])]\n",
    "checkins = pd.merge(checkins, checkin_poi, left_on=\"poi\", right_on=\"id\")\n",
    "full_dataset = pd.merge(\n",
    "    full_dataset, \n",
    "    checkins.groupby(\"square\")[\"user\"].count(), \n",
    "    left_index=True, \n",
    "    right_index=True,\n",
    "    how=\"left\"\n",
    ").rename(columns={\"user\":\"checkins\"})\n",
    "full_dataset[\"checkins\"] = full_dataset[\"checkins\"].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neighbors = [\n",
    "    (0, 1),\n",
    "    (1, 0),\n",
    "    (0, -1),\n",
    "    (-1, 0),\n",
    "    (1, 1),\n",
    "    (-1, -1),\n",
    "    (1, -1),\n",
    "    (-1, 1)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neighbor_datasets = []\n",
    "for n1, n2 in neighbors:\n",
    "    neighbor_idx = full_dataset.apply(lambda x: full_dataset[\n",
    "        (full_dataset[\"label\"] == x[\"label\"]) &\n",
    "        (full_dataset[\"index1\"] == x[\"index1\"] + n1) & \n",
    "        (full_dataset[\"index2\"] == x[\"index2\"] + n2)\n",
    "    ].index, axis=1)\n",
    "    neighbor_idx = neighbor_idx.apply(lambda x: x[0] if len(x) > 0 else np.nan)\n",
    "    neighbor_idx = neighbor_idx[neighbor_idx.notna()]\n",
    "\n",
    "    neighbor_data = full_dataset.loc[neighbor_idx, [c for c in full_dataset.columns[6:] if c != \"label\"]]\n",
    "    neighbor_data.columns = [c + f\"_n{n1},{n2}\" for c in neighbor_data.columns]\n",
    "\n",
    "    neighbor_datasets.append(neighbor_data)\n",
    "full_dataset = pd.concat([full_dataset]+neighbor_datasets, axis=1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset.to_csv(\"squares.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
