{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from osgeo import gdal\n",
    "import geopandas as gpd\n",
    "gdal.SetConfigOption('SHAPE_RESTORE_SHX', 'YES')\n",
    "\n",
    "import pandas as pd\n",
    "import foursquare as fsq\n",
    "import folium\n",
    "import shapely\n",
    "from geopy import distance\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shanghai\n",
    "shape_file = 'shanghai-provjson.shp'\n",
    "# Nanjing\n",
    "top_left = (118.39246295229297, 32.56306709606652) # long, lat\n",
    "bottom_right = (119.22570190410435, 31.261649948659116)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_earth = 6_371_000 # meters\n",
    "def move_lat(lat, d_lat):\n",
    "    lat += d_lat / r_earth * 180 / np.pi\n",
    "    return lat\n",
    "def move_long(long, d_long, lat):\n",
    "    long += d_long / r_earth * 180 / np.pi / np.cos(lat * np.pi / 180)\n",
    "    return long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if shape_file:\n",
    "    geo_df = gpd.read_file(shape_file)\n",
    "\n",
    "    polygons = geo_df.geometry.tolist()\n",
    "    boundary = gpd.GeoSeries(shapely.ops.unary_union(polygons))\n",
    "    long, lat = boundary.at[0].exterior.coords.xy\n",
    "\n",
    "    min_long = min(long)\n",
    "    max_long = max(long)\n",
    "    min_lat = min(lat)\n",
    "    max_lat = max(lat)\n",
    "\n",
    "else:\n",
    "    if top_left is None or bottom_right is None:\n",
    "        raise ValueError(\"Please provide either a shape file or the top left and bottom right coordinates.\")\n",
    "    \n",
    "    min_long, min_lat = top_left\n",
    "    max_long, max_lat = bottom_right\n",
    "\n",
    "long = min_long\n",
    "lat = min_lat\n",
    "\n",
    "columns={\"min_long\":[], \"min_lat\":[], \"max_long\":[], \"max_lat\":[]}\n",
    "\n",
    "while lat <= max_lat:\n",
    "    next_lat = move_lat(lat, 500)\n",
    "\n",
    "    while long <= max_long:\n",
    "        next_long = move_long(long, 500, lat)\n",
    "\n",
    "        columns[\"min_long\"].append(long)\n",
    "        columns[\"min_lat\"].append(lat)\n",
    "        columns[\"max_long\"].append(next_long)\n",
    "        columns[\"max_lat\"].append(next_lat)\n",
    "\n",
    "        long = next_long\n",
    "        \n",
    "    lat = next_lat\n",
    "    long = min_long    \n",
    "\n",
    "squares = pd.DataFrame(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key=\"\"\n",
    "\n",
    "url = \"https://api.foursquare.com/v3/places/search\"\n",
    "headers = {\"accept\": \"application/json\", \"Authorization\": api_key}\n",
    "\n",
    "def poi_scraper(coords, radius=50_000, query=\"7 Days Inn\", category=19014):\n",
    "\n",
    "    if isinstance(category, list):\n",
    "        category = \",\".join([str(c) for c in category])\n",
    "\n",
    "    # Add search parameters here\n",
    "    params={\n",
    "        \"ll\": f\"{coords[0]},{coords[1]}\",\n",
    "        # \"near\": \"Shanghai\",\n",
    "        \"query\": query,\n",
    "        \"radius\": int(radius),\n",
    "        \"limit\": 50,\n",
    "        \"categories\": category # 19014 is the id for hotel\n",
    "    }\n",
    "\n",
    "    s = requests.Session()\n",
    "    retries = Retry(backoff_factor=0.1, status_forcelist=[429, 500, 502, 503, 504])\n",
    "    s.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "    data = json.loads(s.get(url, headers=headers,params=params).text)\n",
    "    \n",
    "    \"\"\" data = []\n",
    "    while \"results\" not in data:\n",
    "        # Request data from foursquare\n",
    "        data = json.loads(requests.get(url, headers=headers,params=params).text)\n",
    "\n",
    "        if \"results\" not in data:\n",
    "            print(data) \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(data[\"results\"])\n",
    "    if len(df) == 0:\n",
    "        return None\n",
    "    df[\"latitude\"] = df[\"geocodes\"].apply(lambda x: x[\"main\"][\"latitude\"])\n",
    "    df[\"longitude\"] = df[\"geocodes\"].apply(lambda x: x[\"main\"][\"longitude\"])\n",
    "    df[\"address\"] = df[\"location\"].apply(lambda x: x[\"formatted_address\"])\n",
    "    df[\"genre\"] = df[\"categories\"].apply(lambda x: [item[\"id\"] for item in x])\n",
    "    df = df.loc[:, [\"name\", \"latitude\", \"longitude\", \"distance\", \"address\", \"genre\"]]    \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POI categories from paper | foursquare categories\n",
    "--- | ---\n",
    "education | 12009,Community and Government > Education\n",
    "scenic spots | 16000,Landmarks and Outdoors\n",
    "sports | 18000,Sports and Recreation\n",
    "commercial spots | 17000,Retail\n",
    "financial services | 11042,Business and Professional Services > Financial Service\n",
    "transport facilities | 19030,Travel and Transportation > Transport Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df = pd.read_csv(\"foursquare_categories.csv\")\n",
    "\n",
    "categories = {\n",
    "    \"education\": (12009,\"Community and Government > Education\"),\n",
    "    \"scenic\": (16000,\"Landmarks and Outdoors\"),\n",
    "    \"sports\": (18000,\"Sports and Recreation\"),\n",
    "    \"commercial\": (17000,\"Retail\"),\n",
    "    \"financial\": (11042,\"Business and Professional Services > Financial Service\"),\n",
    "    \"transport\": (19030,\"Travel and Transportation > Transport Hub\")\n",
    "}\n",
    "\n",
    "city_names = [\"Shanghai\", \"上海\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_scraping(center, radius, squares):\n",
    "    df = poi_scraper(center, radius, query=\"\", category=list(categories.values()))\n",
    "    if df is None:\n",
    "        # print(\"No data for\", center, radius)\n",
    "        return []\n",
    "    \n",
    "    if len(df) > 45:\n",
    "        # print(center, radius, len(df))\n",
    "        return [\n",
    "            parallel_scraping((move_lat(center[0],-radius/(2*np.sqrt(2))),move_long(center[1],-radius/(2*np.sqrt(2)), center[0])), radius/2, squares),\n",
    "            parallel_scraping((move_lat(center[0],-radius/(2*np.sqrt(2))),move_long(center[1], radius/(2*np.sqrt(2)), center[0])), radius/2, squares),\n",
    "            parallel_scraping((move_lat(center[0], radius/(2*np.sqrt(2))),move_long(center[1],-radius/(2*np.sqrt(2)), center[0])), radius/2, squares),\n",
    "            parallel_scraping((move_lat(center[0], radius/(2*np.sqrt(2))),move_long(center[1], radius/(2*np.sqrt(2)), center[0])), radius/2, squares),\n",
    "        ]\n",
    "    \n",
    "    # remove pois outside of the city\n",
    "    df = df[(\n",
    "            pd.DataFrame([df[\"address\"].str.contains(city_names[i]) for i in range(len(city_names))]).transpose().any(axis=1)\n",
    "        )].reset_index(drop=True)\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        return []\n",
    "    \n",
    "    # assign square\n",
    "    df[\"square\"] = df.apply(lambda x:\n",
    "        squares[\n",
    "            (squares[\"min_long\"]<= x[\"longitude\"]) & (squares[\"max_long\"]>x[\"longitude\"]) &\n",
    "            (squares[\"min_lat\"]<= x[\"latitude\"]) & (squares[\"max_lat\"]>x[\"latitude\"])\n",
    "        ].index[0], \n",
    "    axis=1)\n",
    "\n",
    "    # assign category\n",
    "    df[\"category\"] = df.apply(lambda x: [\n",
    "        name \n",
    "        for name, cat in categories.items() \n",
    "        for genre in x[\"genre\"] \n",
    "        if re.match(cat[1],cat_df[cat_df[\"category_id\"]==genre][\"category_label\"].values[0])\n",
    "    ][0], axis=1)\n",
    "\n",
    "    return [df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [04:27<00:00,  8.11s/it]\n"
     ]
    }
   ],
   "source": [
    "squares = squares.assign(**{cat:0 for cat in categories.keys()})\n",
    "\n",
    "grid_size = 100\n",
    "long_step = (max_long - min_long) / grid_size\n",
    "lat_step = (max_lat - min_lat) / grid_size\n",
    "\n",
    "data = Parallel(n_jobs=16)(delayed(parallel_scraping)(\n",
    "    (min_lat + (i+1) * lat_step, min_long + (j+1) * long_step), \n",
    "    int(distance.distance(\n",
    "        (min_lat + (i+1) * lat_step, min_long + (j+1) * long_step),\n",
    "        (min_lat + (i+2) * lat_step, min_long + (j+2) * long_step)\n",
    "    ).m / 2),\n",
    "    squares\n",
    ") for i in tqdm(range(grid_size)) for j in range(grid_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(d):\n",
    "    if isinstance(d[0], list):\n",
    "        flattened = []\n",
    "        for i in d:\n",
    "            if len(i) > 0:\n",
    "                flattened += flatten(i)\n",
    "        return flattened\n",
    "    else:\n",
    "        return d\n",
    "flattened = flatten(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(flattened).reset_index(drop=True).drop_duplicates(subset=[\"name\", \"latitude\", \"longitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"square\"] = df.apply(lambda x: x[\"square\"][0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"category\"] = df.apply(lambda x: x[\"category\"][0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "squares = pd.merge(squares, df.pivot_table(index=\"square\", columns=\"category\", values=\"name\", aggfunc=\"count\", fill_value=0), left_index=True, right_index=True, how=\"outer\", suffixes=(\"\", \"_new\"))\n",
    "\n",
    "for cat in categories.keys():\n",
    "    squares[cat] = squares[cat] + squares[cat+\"_new\"].fillna(0)\n",
    "    squares = squares.drop(columns=cat+\"_new\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "squares[(squares.loc[:, list(categories.keys())]!=0).any(axis=1)].to_csv(\"squares_filtered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "squares.to_csv(\"squares.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
